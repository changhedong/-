{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c535b5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded filepaths: \n",
      "\tmodel_storage/ch5/cbow\\vectorizer.json\n",
      "\tmodel_storage/ch5/cbow\\model.pth\n",
      "Using CUDA: False\n",
      "Loading dataset and creating vectorizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training routine:   0%|                                                                          | 0/5 [00:00<?, ?it/s]\n",
      "split=train:   0%|                                                                            | 0/1984 [00:00<?, ?it/s]\u001b[A\n",
      "split=val:   0%|                                                                               | 0/425 [00:00<?, ?it/s]\u001b[A\n",
      "split=train:   0%|                                                 | 0/1984 [00:00<?, ?it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:   0%|                                         | 4/1984 [00:00<00:50, 39.53it/s, acc=0, epoch=0, loss=9.87]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [1/5]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "split=train:   1%|▏                                       | 12/1984 [00:00<00:34, 57.54it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:   1%|▍                                       | 20/1984 [00:00<00:31, 63.32it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:   1%|▌                                       | 28/1984 [00:00<00:28, 68.34it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:   2%|▋                                       | 35/1984 [00:00<00:28, 67.34it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:   2%|▊                                       | 42/1984 [00:00<00:28, 67.32it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:   2%|▉                                       | 49/1984 [00:00<00:29, 66.01it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:   3%|█▏                                      | 56/1984 [00:00<00:28, 66.92it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:   3%|█▎                                      | 63/1984 [00:00<00:28, 67.06it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:   4%|█▍                                      | 71/1984 [00:01<00:28, 68.00it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:   4%|█▌                                      | 79/1984 [00:01<00:27, 70.07it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:   4%|█▊                                      | 87/1984 [00:01<00:28, 67.02it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:   5%|█▉                                      | 94/1984 [00:01<00:28, 66.47it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:   5%|█▉                                     | 101/1984 [00:01<00:28, 66.58it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:   5%|██                                     | 108/1984 [00:01<00:28, 66.99it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:   6%|██▎                                    | 115/1984 [00:01<00:27, 67.29it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:   6%|██▍                                    | 122/1984 [00:01<00:27, 67.56it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:   7%|██▌                                    | 130/1984 [00:01<00:27, 67.63it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:   7%|██▋                                    | 137/1984 [00:02<00:27, 67.92it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:   7%|██▊                                    | 145/1984 [00:02<00:26, 69.57it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:   8%|██▉                                    | 152/1984 [00:02<00:26, 68.89it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:   8%|███▏                                   | 160/1984 [00:02<00:26, 69.82it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:   8%|███▎                                   | 167/1984 [00:02<00:26, 69.78it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:   9%|███▍                                   | 175/1984 [00:02<00:25, 72.22it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:   9%|███▌                                   | 183/1984 [00:02<00:25, 70.07it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:  10%|███▊                                   | 191/1984 [00:02<00:25, 71.10it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:  10%|███▉                                   | 199/1984 [00:02<00:25, 69.59it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:  10%|████                                   | 207/1984 [00:03<00:24, 71.55it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:  11%|████▏                                  | 215/1984 [00:03<00:25, 69.78it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:  11%|████▍                                  | 224/1984 [00:03<00:24, 72.32it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:  12%|████▌                                  | 232/1984 [00:03<00:25, 69.94it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:  12%|████▋                                  | 240/1984 [00:03<00:24, 70.29it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:  12%|████▉                                  | 248/1984 [00:03<00:25, 68.96it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:  13%|█████                                  | 256/1984 [00:03<00:24, 70.13it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:  13%|█████▏                                 | 264/1984 [00:03<00:24, 70.37it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:  14%|█████▎                                 | 272/1984 [00:03<00:23, 71.95it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:  14%|█████▌                                 | 280/1984 [00:04<00:24, 69.76it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:  15%|█████▋                                 | 288/1984 [00:04<00:23, 71.25it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:  15%|█████▊                                 | 296/1984 [00:04<00:23, 70.56it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:  15%|█████▉                                 | 304/1984 [00:04<00:23, 71.73it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:  16%|██████▏                                | 312/1984 [00:04<00:23, 71.62it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:  16%|██████▎                                | 320/1984 [00:04<00:23, 71.92it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:  17%|██████▍                                | 328/1984 [00:04<00:23, 70.64it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:  17%|██████▌                                | 336/1984 [00:04<00:24, 68.40it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:  17%|██████▊                                | 344/1984 [00:04<00:23, 69.10it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:  18%|██████▉                                | 352/1984 [00:05<00:23, 69.50it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:  18%|███████                                | 360/1984 [00:05<00:23, 70.58it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:  19%|███████▏                               | 368/1984 [00:05<00:23, 69.98it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:  19%|███████▍                               | 376/1984 [00:05<00:22, 70.38it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:  19%|███████▌                               | 384/1984 [00:05<00:22, 70.73it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:  20%|███████▋                               | 392/1984 [00:05<00:22, 70.48it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:  20%|███████▊                               | 400/1984 [00:05<00:22, 71.32it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:  21%|████████                               | 408/1984 [00:05<00:22, 70.84it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:  21%|████████▏                              | 416/1984 [00:05<00:21, 72.28it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:  21%|████████▎                              | 424/1984 [00:06<00:22, 70.17it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:  22%|████████▍                              | 432/1984 [00:06<00:22, 70.12it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:  22%|████████▋                              | 440/1984 [00:06<00:22, 69.61it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:  23%|████████▊                              | 447/1984 [00:06<00:22, 69.70it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:  23%|████████▉                              | 454/1984 [00:06<00:22, 67.37it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:  23%|█████████                              | 461/1984 [00:06<00:22, 68.03it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:  24%|█████████▏                             | 468/1984 [00:06<00:22, 66.87it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:  24%|█████████▎                             | 475/1984 [00:06<00:22, 66.10it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:  24%|█████████▍                             | 482/1984 [00:06<00:23, 64.45it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:  25%|█████████▌                             | 489/1984 [00:07<00:23, 64.71it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:  25%|█████████▊                             | 497/1984 [00:07<00:21, 68.37it/s, acc=0, epoch=0, loss=9.87]\u001b[A\n",
      "split=train:  25%|████████▌                         | 500/1984 [00:07<00:21, 68.37it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  25%|████████▋                         | 504/1984 [00:07<00:22, 64.97it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "split=train:  26%|████████▊                         | 511/1984 [00:07<00:22, 65.46it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  26%|████████▉                         | 519/1984 [00:07<00:21, 68.03it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  27%|█████████                         | 526/1984 [00:07<00:21, 66.66it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  27%|█████████▏                        | 533/1984 [00:07<00:21, 66.09it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  27%|█████████▎                        | 541/1984 [00:07<00:21, 67.11it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  28%|█████████▍                        | 549/1984 [00:07<00:21, 68.05it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  28%|█████████▌                        | 557/1984 [00:08<00:21, 67.85it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  28%|█████████▋                        | 565/1984 [00:08<00:20, 68.92it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  29%|█████████▊                        | 573/1984 [00:08<00:20, 69.64it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  29%|█████████▉                        | 580/1984 [00:08<00:20, 68.94it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  30%|██████████                        | 587/1984 [00:08<00:20, 68.26it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  30%|██████████▏                       | 595/1984 [00:08<00:19, 69.58it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  30%|██████████▎                       | 602/1984 [00:08<00:19, 69.19it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  31%|██████████▍                       | 609/1984 [00:08<00:19, 68.98it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  31%|██████████▌                       | 617/1984 [00:08<00:19, 70.33it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  32%|██████████▋                       | 625/1984 [00:09<00:18, 72.79it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  32%|██████████▊                       | 633/1984 [00:09<00:19, 68.40it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  32%|██████████▉                       | 640/1984 [00:09<00:19, 67.58it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  33%|███████████                       | 647/1984 [00:09<00:19, 67.42it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  33%|███████████▏                      | 655/1984 [00:09<00:19, 68.32it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  33%|███████████▎                      | 663/1984 [00:09<00:18, 70.04it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  34%|███████████▍                      | 671/1984 [00:09<00:18, 69.46it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  34%|███████████▋                      | 679/1984 [00:09<00:18, 70.01it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  35%|███████████▊                      | 687/1984 [00:09<00:18, 69.84it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  35%|███████████▉                      | 694/1984 [00:10<00:18, 69.30it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  35%|████████████                      | 702/1984 [00:10<00:18, 70.44it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  36%|████████████▏                     | 710/1984 [00:10<00:17, 71.30it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  36%|████████████▎                     | 718/1984 [00:10<00:18, 68.45it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  37%|████████████▍                     | 725/1984 [00:10<00:18, 67.03it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  37%|████████████▌                     | 732/1984 [00:10<00:18, 66.96it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  37%|████████████▋                     | 740/1984 [00:10<00:18, 68.37it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  38%|████████████▊                     | 747/1984 [00:10<00:18, 66.52it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  38%|████████████▉                     | 754/1984 [00:10<00:18, 66.05it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  38%|█████████████                     | 761/1984 [00:11<00:18, 64.61it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  39%|█████████████▏                    | 768/1984 [00:11<00:18, 65.63it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  39%|█████████████▎                    | 775/1984 [00:11<00:18, 66.11it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  39%|█████████████▍                    | 783/1984 [00:11<00:17, 67.68it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  40%|█████████████▌                    | 790/1984 [00:11<00:17, 67.34it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  40%|█████████████▋                    | 798/1984 [00:11<00:17, 68.21it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  41%|█████████████▊                    | 805/1984 [00:11<00:17, 68.44it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  41%|█████████████▉                    | 813/1984 [00:11<00:16, 69.06it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  41%|██████████████                    | 820/1984 [00:11<00:16, 68.92it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  42%|██████████████▏                   | 827/1984 [00:12<00:16, 68.39it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  42%|██████████████▎                   | 835/1984 [00:12<00:16, 70.07it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  42%|██████████████▍                   | 843/1984 [00:12<00:17, 66.62it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  43%|██████████████▌                   | 850/1984 [00:12<00:16, 67.17it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  43%|██████████████▋                   | 857/1984 [00:12<00:16, 67.24it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  44%|██████████████▊                   | 864/1984 [00:12<00:16, 67.39it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  44%|██████████████▉                   | 871/1984 [00:12<00:16, 67.43it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  44%|███████████████                   | 878/1984 [00:12<00:16, 67.73it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  45%|███████████████▏                  | 886/1984 [00:12<00:15, 69.67it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  45%|███████████████▎                  | 893/1984 [00:13<00:15, 68.90it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  45%|███████████████▍                  | 900/1984 [00:13<00:16, 67.49it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  46%|███████████████▌                  | 907/1984 [00:13<00:16, 66.50it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  46%|███████████████▋                  | 915/1984 [00:13<00:15, 68.84it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  46%|███████████████▊                  | 922/1984 [00:13<00:15, 68.29it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  47%|███████████████▉                  | 929/1984 [00:13<00:15, 67.07it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  47%|████████████████                  | 936/1984 [00:13<00:15, 67.38it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  48%|████████████████▏                 | 944/1984 [00:13<00:15, 68.62it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  48%|████████████████▎                 | 952/1984 [00:13<00:14, 69.59it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  48%|████████████████▍                 | 959/1984 [00:13<00:15, 68.10it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  49%|████████████████▌                 | 966/1984 [00:14<00:15, 67.69it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  49%|████████████████▋                 | 974/1984 [00:14<00:14, 69.63it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  49%|████████████████▊                 | 981/1984 [00:14<00:14, 69.23it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  50%|████████████████▉                 | 988/1984 [00:14<00:14, 68.48it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n",
      "split=train:  50%|█████████████████                 | 995/1984 [00:14<00:14, 68.25it/s, acc=0.0811, epoch=0, loss=9.57]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "split=train:  50%|█████████████████▏                | 1000/1984 [00:14<00:14, 68.25it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  51%|█████████████████▏                | 1002/1984 [00:14<00:14, 68.46it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  51%|█████████████████▎                | 1009/1984 [00:14<00:14, 68.30it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  51%|█████████████████▍                | 1016/1984 [00:14<00:14, 68.19it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  52%|█████████████████▌                | 1023/1984 [00:14<00:14, 68.27it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  52%|█████████████████▋                | 1030/1984 [00:15<00:14, 67.97it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  52%|█████████████████▊                | 1038/1984 [00:15<00:13, 70.64it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  53%|█████████████████▉                | 1046/1984 [00:15<00:13, 69.49it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  53%|██████████████████                | 1053/1984 [00:15<00:13, 68.69it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  53%|██████████████████▏               | 1060/1984 [00:15<00:13, 68.79it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  54%|██████████████████▎               | 1067/1984 [00:15<00:13, 67.47it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  54%|██████████████████▍               | 1074/1984 [00:15<00:13, 66.46it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  54%|██████████████████▌               | 1081/1984 [00:15<00:13, 65.75it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  55%|██████████████████▋               | 1089/1984 [00:15<00:13, 67.69it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  55%|██████████████████▊               | 1096/1984 [00:16<00:13, 67.69it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  56%|██████████████████▉               | 1103/1984 [00:16<00:12, 67.84it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  56%|███████████████████               | 1110/1984 [00:16<00:13, 66.56it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  56%|███████████████████▏              | 1117/1984 [00:16<00:12, 67.06it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  57%|███████████████████▎              | 1124/1984 [00:16<00:12, 67.31it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  57%|███████████████████▍              | 1132/1984 [00:16<00:12, 68.25it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  57%|███████████████████▌              | 1139/1984 [00:16<00:12, 66.73it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  58%|███████████████████▋              | 1146/1984 [00:16<00:12, 67.18it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  58%|███████████████████▊              | 1153/1984 [00:16<00:12, 67.62it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  58%|███████████████████▉              | 1160/1984 [00:16<00:12, 67.40it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  59%|███████████████████▉              | 1167/1984 [00:17<00:12, 68.04it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  59%|████████████████████▏             | 1175/1984 [00:17<00:11, 68.51it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  60%|████████████████████▎             | 1182/1984 [00:17<00:11, 68.34it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  60%|████████████████████▍             | 1190/1984 [00:17<00:11, 68.70it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  60%|████████████████████▌             | 1197/1984 [00:17<00:11, 66.97it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  61%|████████████████████▋             | 1204/1984 [00:17<00:11, 65.97it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  61%|████████████████████▊             | 1211/1984 [00:17<00:11, 65.93it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  61%|████████████████████▊             | 1218/1984 [00:17<00:11, 65.65it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  62%|████████████████████▉             | 1225/1984 [00:17<00:11, 64.68it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  62%|█████████████████████             | 1232/1984 [00:18<00:11, 64.01it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  62%|█████████████████████▏            | 1239/1984 [00:18<00:11, 63.31it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  63%|█████████████████████▎            | 1246/1984 [00:18<00:11, 65.14it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  63%|█████████████████████▍            | 1254/1984 [00:18<00:10, 67.54it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  64%|█████████████████████▌            | 1261/1984 [00:18<00:10, 66.51it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  64%|█████████████████████▋            | 1268/1984 [00:18<00:11, 64.38it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  64%|█████████████████████▊            | 1275/1984 [00:18<00:11, 63.85it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  65%|█████████████████████▉            | 1282/1984 [00:18<00:10, 64.49it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  65%|██████████████████████            | 1289/1984 [00:18<00:10, 64.48it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  65%|██████████████████████▏           | 1296/1984 [00:19<00:10, 65.62it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  66%|██████████████████████▎           | 1303/1984 [00:19<00:10, 64.89it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  66%|██████████████████████▍           | 1310/1984 [00:19<00:10, 62.81it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  66%|██████████████████████▌           | 1317/1984 [00:19<00:10, 64.40it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  67%|██████████████████████▋           | 1324/1984 [00:19<00:10, 63.10it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  67%|██████████████████████▊           | 1331/1984 [00:19<00:10, 63.77it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  67%|██████████████████████▉           | 1338/1984 [00:19<00:09, 64.76it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  68%|███████████████████████           | 1345/1984 [00:19<00:09, 65.30it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  68%|███████████████████████▏          | 1352/1984 [00:19<00:09, 63.22it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  68%|███████████████████████▎          | 1359/1984 [00:20<00:09, 63.53it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  69%|███████████████████████▍          | 1367/1984 [00:20<00:09, 66.08it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  69%|███████████████████████▌          | 1374/1984 [00:20<00:09, 66.42it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  70%|███████████████████████▋          | 1382/1984 [00:20<00:08, 68.15it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  70%|███████████████████████▊          | 1389/1984 [00:20<00:08, 67.43it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  70%|███████████████████████▉          | 1396/1984 [00:20<00:08, 65.58it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  71%|████████████████████████          | 1404/1984 [00:20<00:08, 68.89it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  71%|████████████████████████▏         | 1411/1984 [00:20<00:08, 67.04it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  72%|████████████████████████▎         | 1419/1984 [00:20<00:08, 68.02it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  72%|████████████████████████▍         | 1426/1984 [00:20<00:08, 67.41it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  72%|████████████████████████▌         | 1433/1984 [00:21<00:08, 67.02it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  73%|████████████████████████▋         | 1440/1984 [00:21<00:08, 66.17it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  73%|████████████████████████▊         | 1447/1984 [00:21<00:08, 66.68it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  73%|████████████████████████▉         | 1454/1984 [00:21<00:08, 65.56it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  74%|█████████████████████████         | 1461/1984 [00:21<00:07, 65.42it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "split=train:  74%|█████████████████████████▏        | 1468/1984 [00:21<00:07, 64.54it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  74%|█████████████████████████▎        | 1476/1984 [00:21<00:07, 66.57it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  75%|█████████████████████████▍        | 1483/1984 [00:21<00:07, 67.13it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  75%|█████████████████████████▌        | 1490/1984 [00:21<00:07, 67.43it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  76%|█████████████████████████▋        | 1498/1984 [00:22<00:06, 69.72it/s, acc=0.415, epoch=0, loss=9.23]\u001b[A\n",
      "split=train:  76%|█████████████████████████▋        | 1500/1984 [00:22<00:06, 69.72it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  76%|█████████████████████████▊        | 1505/1984 [00:22<00:07, 67.77it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  76%|█████████████████████████▉        | 1512/1984 [00:22<00:07, 65.73it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  77%|██████████████████████████        | 1519/1984 [00:22<00:07, 65.13it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  77%|██████████████████████████▏       | 1526/1984 [00:22<00:07, 63.70it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  77%|██████████████████████████▎       | 1533/1984 [00:22<00:06, 64.77it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  78%|██████████████████████████▍       | 1540/1984 [00:22<00:06, 66.06it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  78%|██████████████████████████▌       | 1548/1984 [00:22<00:06, 67.70it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  78%|██████████████████████████▋       | 1555/1984 [00:22<00:06, 67.48it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  79%|██████████████████████████▊       | 1562/1984 [00:23<00:06, 65.40it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  79%|██████████████████████████▉       | 1569/1984 [00:23<00:06, 64.85it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  79%|███████████████████████████       | 1576/1984 [00:23<00:06, 65.92it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  80%|███████████████████████████▏      | 1583/1984 [00:23<00:06, 66.20it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  80%|███████████████████████████▎      | 1591/1984 [00:23<00:05, 67.92it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  81%|███████████████████████████▍      | 1599/1984 [00:23<00:05, 69.50it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  81%|███████████████████████████▌      | 1606/1984 [00:23<00:05, 68.81it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  81%|███████████████████████████▋      | 1614/1984 [00:23<00:05, 69.50it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  82%|███████████████████████████▊      | 1622/1984 [00:23<00:05, 69.60it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  82%|███████████████████████████▉      | 1630/1984 [00:24<00:05, 69.56it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  83%|████████████████████████████      | 1638/1984 [00:24<00:04, 69.63it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  83%|████████████████████████████▏     | 1646/1984 [00:24<00:04, 71.52it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  83%|████████████████████████████▎     | 1654/1984 [00:24<00:04, 68.21it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  84%|████████████████████████████▍     | 1662/1984 [00:24<00:04, 69.27it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  84%|████████████████████████████▌     | 1670/1984 [00:24<00:04, 69.40it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  85%|████████████████████████████▊     | 1678/1984 [00:24<00:04, 70.30it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  85%|████████████████████████████▉     | 1686/1984 [00:24<00:04, 70.14it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  85%|█████████████████████████████     | 1694/1984 [00:24<00:04, 71.16it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  86%|█████████████████████████████▏    | 1702/1984 [00:25<00:03, 72.59it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  86%|█████████████████████████████▎    | 1710/1984 [00:25<00:03, 70.95it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  87%|█████████████████████████████▍    | 1718/1984 [00:25<00:03, 67.51it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  87%|█████████████████████████████▌    | 1726/1984 [00:25<00:03, 69.51it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  87%|█████████████████████████████▋    | 1733/1984 [00:25<00:03, 69.02it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  88%|█████████████████████████████▊    | 1741/1984 [00:25<00:03, 68.90it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  88%|█████████████████████████████▉    | 1748/1984 [00:25<00:03, 68.11it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  89%|██████████████████████████████    | 1756/1984 [00:25<00:03, 68.64it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  89%|██████████████████████████████▏   | 1764/1984 [00:25<00:03, 68.12it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  89%|██████████████████████████████▎   | 1771/1984 [00:26<00:03, 67.91it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  90%|██████████████████████████████▍   | 1778/1984 [00:26<00:03, 66.79it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  90%|██████████████████████████████▌   | 1786/1984 [00:26<00:02, 67.64it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  90%|██████████████████████████████▋   | 1794/1984 [00:26<00:02, 69.14it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  91%|██████████████████████████████▉   | 1802/1984 [00:26<00:02, 68.15it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  91%|███████████████████████████████   | 1810/1984 [00:26<00:02, 69.27it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  92%|███████████████████████████████▏  | 1818/1984 [00:26<00:02, 69.67it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  92%|███████████████████████████████▎  | 1825/1984 [00:26<00:02, 67.10it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  92%|███████████████████████████████▍  | 1832/1984 [00:26<00:02, 67.40it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  93%|███████████████████████████████▌  | 1840/1984 [00:27<00:02, 69.87it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  93%|███████████████████████████████▋  | 1847/1984 [00:27<00:01, 69.23it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  93%|███████████████████████████████▊  | 1854/1984 [00:27<00:01, 68.92it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  94%|███████████████████████████████▉  | 1861/1984 [00:27<00:01, 67.31it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  94%|████████████████████████████████  | 1869/1984 [00:27<00:01, 68.13it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  95%|████████████████████████████████▏ | 1876/1984 [00:27<00:01, 65.10it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  95%|████████████████████████████████▎ | 1883/1984 [00:27<00:01, 64.50it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  95%|████████████████████████████████▍ | 1890/1984 [00:27<00:01, 65.74it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  96%|████████████████████████████████▌ | 1897/1984 [00:27<00:01, 66.59it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  96%|████████████████████████████████▋ | 1904/1984 [00:28<00:01, 65.38it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  96%|████████████████████████████████▋ | 1911/1984 [00:28<00:01, 66.38it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  97%|████████████████████████████████▊ | 1918/1984 [00:28<00:01, 65.93it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  97%|█████████████████████████████████ | 1926/1984 [00:28<00:00, 68.19it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  97%|█████████████████████████████████▏| 1933/1984 [00:28<00:00, 65.68it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  98%|█████████████████████████████████▎| 1941/1984 [00:28<00:00, 67.60it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  98%|█████████████████████████████████▍| 1948/1984 [00:28<00:00, 66.95it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "split=train:  99%|█████████████████████████████████▌| 1955/1984 [00:28<00:00, 66.08it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  99%|█████████████████████████████████▌| 1962/1984 [00:28<00:00, 66.90it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train:  99%|█████████████████████████████████▋| 1969/1984 [00:29<00:00, 67.12it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train: 100%|█████████████████████████████████▊| 1976/1984 [00:29<00:00, 66.89it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=train: 100%|█████████████████████████████████▉| 1983/1984 [00:29<00:00, 66.91it/s, acc=0.908, epoch=0, loss=8.98]\u001b[A\n",
      "split=val:   0%|                                                 | 0/425 [00:29<?, ?it/s, acc=6.25, epoch=0, loss=7.56]\u001b[A\n",
      "split=val:   0%|                                       | 1/425 [00:29<3:26:42, 29.25s/it, acc=6.25, epoch=0, loss=7.56]\u001b[A\n",
      "split=val:   5%|██                                      | 22/425 [00:29<06:22,  1.05it/s, acc=6.25, epoch=0, loss=7.56]\u001b[A\n",
      "split=val:  10%|████                                    | 43/425 [00:29<02:33,  2.49it/s, acc=6.25, epoch=0, loss=7.56]\u001b[A\n",
      "split=val:  15%|██████                                  | 65/425 [00:29<01:18,  4.61it/s, acc=6.25, epoch=0, loss=7.56]\u001b[A\n",
      "split=val:  20%|████████                                | 86/425 [00:29<00:45,  7.42it/s, acc=6.25, epoch=0, loss=7.56]\u001b[A\n",
      "split=val:  25%|█████████▉                             | 108/425 [00:29<00:27, 11.48it/s, acc=6.25, epoch=0, loss=7.56]\u001b[A\n",
      "split=val:  30%|███████████▊                           | 129/425 [00:29<00:17, 16.77it/s, acc=6.25, epoch=0, loss=7.56]\u001b[A\n",
      "split=val:  36%|██████████████                         | 153/425 [00:29<00:10, 24.83it/s, acc=6.25, epoch=0, loss=7.56]\u001b[A\n",
      "split=val:  41%|███████████████▉                       | 174/425 [00:30<00:07, 33.85it/s, acc=6.25, epoch=0, loss=7.56]\u001b[A\n",
      "split=val:  46%|██████████████████                     | 197/425 [00:30<00:04, 46.68it/s, acc=6.25, epoch=0, loss=7.56]\u001b[A\n",
      "split=val:  52%|████████████████████                   | 219/425 [00:30<00:03, 61.18it/s, acc=6.25, epoch=0, loss=7.56]\u001b[A\n",
      "split=val:  57%|██████████████████████▏                | 242/425 [00:30<00:02, 79.13it/s, acc=6.25, epoch=0, loss=7.56]\u001b[A\n",
      "split=val:  62%|████████████████████████▏              | 264/425 [00:30<00:01, 97.37it/s, acc=6.25, epoch=0, loss=7.56]\u001b[A\n",
      "split=val:  67%|█████████████████████████▌            | 286/425 [00:30<00:01, 115.77it/s, acc=6.25, epoch=0, loss=7.56]\u001b[A\n",
      "split=val:  72%|███████████████████████████▍          | 307/425 [00:30<00:00, 131.79it/s, acc=6.25, epoch=0, loss=7.56]\u001b[A\n",
      "split=val:  77%|█████████████████████████████▎        | 328/425 [00:30<00:00, 145.63it/s, acc=6.25, epoch=0, loss=7.56]\u001b[A\n",
      "split=val:  82%|███████████████████████████████▏      | 349/425 [00:30<00:00, 159.75it/s, acc=6.25, epoch=0, loss=7.56]\u001b[A\n",
      "split=val:  87%|█████████████████████████████████     | 370/425 [00:31<00:00, 170.92it/s, acc=6.25, epoch=0, loss=7.56]\u001b[A\n",
      "split=val:  92%|██████████████████████████████████▉   | 391/425 [00:31<00:00, 180.21it/s, acc=6.25, epoch=0, loss=7.56]\u001b[A\n",
      "training routine:  20%|█████████████▏                                                    | 1/5 [00:31<02:05, 31.31s/it]\u001b[A\n",
      "split=train:   0%|                                         | 0/1984 [00:31<00:29, 66.91it/s, acc=0, epoch=1, loss=8.41]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [2/5]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "split=train:  25%|█████████                           | 500/1984 [00:38<00:22, 66.91it/s, acc=5.62, epoch=1, loss=7.82]\u001b[A\n",
      "split=train:  30%|██████████▊                         | 595/1984 [00:40<00:20, 66.91it/s, acc=5.62, epoch=1, loss=7.82]\u001b[A\n",
      "split=train:  50%|█████████████████▋                 | 1000/1984 [00:45<00:14, 66.91it/s, acc=6.15, epoch=1, loss=7.73]\u001b[A\n",
      "split=val:   0%|                                        | 0/425 [00:50<00:02, 190.60it/s, acc=6.25, epoch=0, loss=7.56]\u001b[A\n",
      "split=train:  76%|██████████████████████████▍        | 1500/1984 [00:52<00:07, 66.91it/s, acc=6.77, epoch=1, loss=7.66]\u001b[A\n",
      "split=train: 100%|███████████████████████████████████| 1984/1984 [01:00<00:00,  1.80s/it, acc=6.77, epoch=1, loss=7.66]\u001b[A\n",
      "split=val:   0%|                                        | 0/425 [01:00<00:02, 190.60it/s, acc=9.38, epoch=1, loss=7.72]\u001b[A\n",
      "training routine:  40%|██████████████████████████▍                                       | 2/5 [01:02<01:33, 31.24s/it]\u001b[A\n",
      "split=train:   0%|                                      | 0/1984 [01:02<59:27,  1.80s/it, acc=9.38, epoch=2, loss=7.25]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [3/5]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "split=train:  25%|█████████                           | 500/1984 [01:09<44:28,  1.80s/it, acc=8.64, epoch=2, loss=7.25]\u001b[A\n",
      "split=train:  50%|█████████████████▋                 | 1000/1984 [01:17<29:29,  1.80s/it, acc=8.91, epoch=2, loss=7.21]\u001b[A\n",
      "split=train:  76%|██████████████████████████▍        | 1500/1984 [01:25<14:30,  1.80s/it, acc=9.15, epoch=2, loss=7.17]\u001b[A\n",
      "split=val:   0%|                                         | 0/425 [01:32<04:15,  1.66it/s, acc=6.25, epoch=2, loss=6.48]\u001b[A\n",
      "training routine:  60%|███████████████████████████████████████▌                          | 3/5 [01:34<01:03, 31.57s/it]\u001b[A\n",
      "split=train:   0%|                                       | 0/1984 [01:34<59:27,  1.80s/it, acc=21.9, epoch=3, loss=6.6]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [4/5]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "split=train:  25%|█████████                           | 500/1984 [01:42<44:28,  1.80s/it, acc=9.77, epoch=3, loss=6.88]\u001b[A\n",
      "split=train:  50%|█████████████████▋                 | 1000/1984 [01:49<29:29,  1.80s/it, acc=9.88, epoch=3, loss=6.89]\u001b[A\n",
      "split=train:  76%|██████████████████████████▍        | 1500/1984 [01:57<14:30,  1.80s/it, acc=10.2, epoch=3, loss=6.87]\u001b[A\n",
      "split=val:   0%|                                         | 0/425 [02:04<10:09,  1.43s/it, acc=15.6, epoch=3, loss=6.41]\u001b[A\n",
      "training routine:  80%|████████████████████████████████████████████████████▊             | 4/5 [02:06<00:31, 31.91s/it]\u001b[A\n",
      "split=train:   0%|                                      | 0/1984 [02:06<59:27,  1.80s/it, acc=9.38, epoch=4, loss=6.72]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [5/5]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "split=train:  25%|█████████▎                           | 500/1984 [02:14<44:28,  1.80s/it, acc=10.8, epoch=4, loss=6.7]\u001b[A\n",
      "split=train:  50%|█████████████████▋                 | 1000/1984 [02:21<29:29,  1.80s/it, acc=10.9, epoch=4, loss=6.68]\u001b[A\n",
      "split=train:  76%|███████████████████████████▉         | 1500/1984 [02:29<14:30,  1.80s/it, acc=11, epoch=4, loss=6.66]\u001b[A\n",
      "split=val:   0%|                                         | 0/425 [02:36<18:10,  2.57s/it, acc=9.38, epoch=4, loss=6.26]\u001b[A\n",
      "training routine: 100%|██████████████████████████████████████████████████████████████████| 5/5 [02:38<00:00, 31.87s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 7.759905684976017;\n",
      "Test Accuracy: 10.558823529411775\n",
      "...[7.85] - saw\n",
      "...[7.90] - kid\n",
      "...[7.94] - cares\n",
      "...[7.95] - ultimately\n",
      "...[7.96] - truly\n",
      "...[7.97] - confused\n",
      "=======frankenstein=======\n",
      "...[7.44] - prejudiced\n",
      "...[7.78] - shrivelled\n",
      "...[7.81] - gush\n",
      "...[7.81] - enslaved\n",
      "...[7.82] - men\n",
      "...[7.83] - liable\n",
      "=======monster=======\n",
      "...[7.85] - saw\n",
      "...[7.90] - kid\n",
      "...[7.94] - cares\n",
      "...[7.95] - ultimately\n",
      "...[7.96] - truly\n",
      "...[7.97] - confused\n",
      "=======science=======\n",
      "...[7.06] - impression\n",
      "...[7.08] - mutual\n",
      "...[7.14] - mist\n",
      "...[7.17] - darkened\n",
      "...[7.27] - swelling\n",
      "...[7.38] - tempted\n",
      "=======sickness=======\n",
      "...[6.31] - while\n",
      "...[6.55] - literally\n",
      "...[6.61] - probabilities\n",
      "...[6.62] - foundations\n",
      "...[6.64] - awoke\n",
      "...[6.70] - consoles\n",
      "=======lonely=======\n",
      "...[6.93] - moonlight\n",
      "...[6.94] - unveiled\n",
      "...[7.15] - ought\n",
      "...[7.25] - heartily\n",
      "...[7.26] - bed\n",
      "...[7.29] - undiscovered\n",
      "=======happy=======\n",
      "...[6.42] - bottom\n",
      "...[6.50] - injury\n",
      "...[6.58] - chimney\n",
      "...[6.62] - chivalry\n",
      "...[6.64] - evening\n",
      "...[6.65] - lingered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class Vocabulary(object):\n",
    "    \"\"\"Class to process text and extract vocabulary for mapping\"\"\"\n",
    "\n",
    "    def __init__(self, token_to_idx=None, mask_token=\"<MASK>\", add_unk=True, unk_token=\"<UNK>\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_to_idx (dict): a pre-existing map of tokens to indices\n",
    "            mask_token (str): the MASK token to add into the Vocabulary; indicates\n",
    "                a position that will not be used in updating the model's parameters\n",
    "            add_unk (bool): a flag that indicates whether to add the UNK token\n",
    "            unk_token (str): the UNK token to add into the Vocabulary\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "\n",
    "        self._idx_to_token = {idx: token\n",
    "                              for token, idx in self._token_to_idx.items()}\n",
    "\n",
    "        self._add_unk = add_unk\n",
    "        self._unk_token = unk_token\n",
    "        self._mask_token = mask_token\n",
    "\n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = -1\n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(unk_token)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        \"\"\" returns a dictionary that can be serialized \"\"\"\n",
    "        return {'token_to_idx': self._token_to_idx,\n",
    "                'add_unk': self._add_unk,\n",
    "                'unk_token': self._unk_token,\n",
    "                'mask_token': self._mask_token}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\" instantiates the Vocabulary from a serialized dictionary \"\"\"\n",
    "        return cls(**contents)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        \"\"\"Update mapping dicts based on the token.\n",
    "\n",
    "        Args:\n",
    "            token (str): the item to add into the Vocabulary\n",
    "        Returns:\n",
    "            index (int): the integer corresponding to the token\n",
    "        \"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "\n",
    "    def add_many(self, tokens):\n",
    "        \"\"\"Add a list of tokens into the Vocabulary\n",
    "\n",
    "        Args:\n",
    "            tokens (list): a list of string tokens\n",
    "        Returns:\n",
    "            indices (list): a list of indices corresponding to the tokens\n",
    "        \"\"\"\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token\n",
    "          or the UNK index if token isn't present.\n",
    "\n",
    "        Args:\n",
    "            token (str): the token to look up\n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        Notes:\n",
    "            `unk_index` needs to be >=0 (having been added into the Vocabulary)\n",
    "              for the UNK functionality\n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        \"\"\"Return the token associated with the index\n",
    "\n",
    "        Args:\n",
    "            index (int): the index to look up\n",
    "        Returns:\n",
    "            token (str): the token corresponding to the index\n",
    "        Raises:\n",
    "            KeyError: if the index is not in the Vocabulary\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)\n",
    "\n",
    "\n",
    "class CBOWVectorizer(object):\n",
    "    \"\"\" The Vectorizer which coordinates the Vocabularies and puts them to use\"\"\"\n",
    "\n",
    "    def __init__(self, cbow_vocab):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cbow_vocab (Vocabulary): maps words to integers\n",
    "        \"\"\"\n",
    "        self.cbow_vocab = cbow_vocab\n",
    "\n",
    "    def vectorize(self, context, vector_length=-1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            context (str): the string of words separated by a space\n",
    "            vector_length (int): an argument for forcing the length of index vector\n",
    "        \"\"\"\n",
    "\n",
    "        indices = [self.cbow_vocab.lookup_token(token) for token in context.split(' ')]\n",
    "        if vector_length < 0:\n",
    "            vector_length = len(indices)\n",
    "\n",
    "        out_vector = np.zeros(vector_length, dtype=np.int64)\n",
    "        out_vector[:len(indices)] = indices\n",
    "        out_vector[len(indices):] = self.cbow_vocab.mask_index\n",
    "\n",
    "        return out_vector\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, cbow_df):\n",
    "        \"\"\"Instantiate the vectorizer from the dataset dataframe\n",
    "\n",
    "        Args:\n",
    "            cbow_df (pandas.DataFrame): the target dataset\n",
    "        Returns:\n",
    "            an instance of the CBOWVectorizer\n",
    "        \"\"\"\n",
    "        cbow_vocab = Vocabulary()\n",
    "        for index, row in cbow_df.iterrows():\n",
    "            for token in row.context.split(' '):\n",
    "                cbow_vocab.add_token(token)\n",
    "            cbow_vocab.add_token(row.target)\n",
    "\n",
    "        return cls(cbow_vocab)\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        cbow_vocab = \\\n",
    "            Vocabulary.from_serializable(contents['cbow_vocab'])\n",
    "        return cls(cbow_vocab=cbow_vocab)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {'cbow_vocab': self.cbow_vocab.to_serializable()}\n",
    "\n",
    "\n",
    "class CBOWDataset(Dataset):\n",
    "    def __init__(self, cbow_df, vectorizer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cbow_df (pandas.DataFrame): the dataset\n",
    "            vectorizer (CBOWVectorizer): vectorizer instatiated from dataset\n",
    "        \"\"\"\n",
    "        self.cbow_df = cbow_df\n",
    "        self._vectorizer = vectorizer\n",
    "\n",
    "        measure_len = lambda context: len(context.split(\" \"))\n",
    "        self._max_seq_length = max(map(measure_len, cbow_df.context))\n",
    "\n",
    "        self.train_df = self.cbow_df[self.cbow_df.split == 'train']\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.cbow_df[self.cbow_df.split == 'val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.cbow_df[self.cbow_df.split == 'test']\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                             'val': (self.val_df, self.validation_size),\n",
    "                             'test': (self.test_df, self.test_size)}\n",
    "\n",
    "        self.set_split('train')\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, cbow_csv):\n",
    "        \"\"\"Load dataset and make a new vectorizer from scratch\n",
    "\n",
    "        Args:\n",
    "            cbow_csv (str): location of the dataset\n",
    "        Returns:\n",
    "            an instance of CBOWDataset\n",
    "        \"\"\"\n",
    "        cbow_df = pd.read_csv(cbow_csv)\n",
    "        train_cbow_df = cbow_df[cbow_df.split == 'train']\n",
    "        return cls(cbow_df, CBOWVectorizer.from_dataframe(train_cbow_df))\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, cbow_csv, vectorizer_filepath):\n",
    "        \"\"\"Load dataset and the corresponding vectorizer.\n",
    "        Used in the case in the vectorizer has been cached for re-use\n",
    "\n",
    "        Args:\n",
    "            cbow_csv (str): location of the dataset\n",
    "            vectorizer_filepath (str): location of the saved vectorizer\n",
    "        Returns:\n",
    "            an instance of CBOWDataset\n",
    "        \"\"\"\n",
    "        cbow_df = pd.read_csv(cbow_csv)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(cbow_df, vectorizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        \"\"\"a static method for loading the vectorizer from file\n",
    "\n",
    "        Args:\n",
    "            vectorizer_filepath (str): the location of the serialized vectorizer\n",
    "        Returns:\n",
    "            an instance of CBOWVectorizer\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return CBOWVectorizer.from_serializable(json.load(fp))\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        \"\"\"saves the vectorizer to disk using json\n",
    "\n",
    "        Args:\n",
    "            vectorizer_filepath (str): the location to save the vectorizer\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        \"\"\" returns the vectorizer \"\"\"\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        \"\"\" selects the splits in the dataset using a column in the dataframe \"\"\"\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"the primary entry point method for PyTorch datasets\n",
    "\n",
    "        Args:\n",
    "            index (int): the index to the data point\n",
    "        Returns:\n",
    "            a dictionary holding the data point's features (x_data) and label (y_target)\n",
    "        \"\"\"\n",
    "        row = self._target_df.iloc[index]\n",
    "\n",
    "        context_vector = \\\n",
    "            self._vectorizer.vectorize(row.context, self._max_seq_length)\n",
    "        target_index = self._vectorizer.cbow_vocab.lookup_token(row.target)\n",
    "\n",
    "        return {'x_data': context_vector,\n",
    "                'y_target': target_index}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\"Given a batch size, return the number of batches in the dataset\n",
    "\n",
    "        Args:\n",
    "            batch_size (int)\n",
    "        Returns:\n",
    "            number of batches in the dataset\n",
    "        \"\"\"\n",
    "        return len(self) // batch_size\n",
    "\n",
    "\n",
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    A generator function which wraps the PyTorch DataLoader. It will\n",
    "      ensure each tensor is on the write device location.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict\n",
    "\n",
    "\n",
    "class CBOWClassifier(nn.Module):  # Simplified cbow Model\n",
    "    def __init__(self, vocabulary_size, embedding_size, padding_idx=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocabulary_size (int): number of vocabulary items, controls the\n",
    "                number of embeddings and prediction vector size\n",
    "            embedding_size (int): size of the embeddings\n",
    "            padding_idx (int): default 0; Embedding will not use this index\n",
    "        \"\"\"\n",
    "        super(CBOWClassifier, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocabulary_size,\n",
    "                                      embedding_dim=embedding_size,\n",
    "                                      padding_idx=padding_idx)\n",
    "        self.fc1 = nn.Linear(in_features=embedding_size,\n",
    "                             out_features=vocabulary_size)\n",
    "\n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        \"\"\"The forward pass of the classifier\n",
    "\n",
    "        Args:\n",
    "            x_in (torch.Tensor): an input data tensor.\n",
    "                x_in.shape should be (batch, input_dim)\n",
    "            apply_softmax (bool): a flag for the softmax activation\n",
    "                should be false if used with the Cross Entropy losses\n",
    "        Returns:\n",
    "            the resulting tensor. tensor.shape should be (batch, output_dim)\n",
    "        \"\"\"\n",
    "        x_embedded_sum = F.dropout(self.embedding(x_in).sum(dim=1), 0.3)\n",
    "        y_out = self.fc1(x_embedded_sum)\n",
    "\n",
    "        if apply_softmax:\n",
    "            y_out = F.softmax(y_out, dim=1)\n",
    "\n",
    "        return y_out\n",
    "\n",
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': args.model_state_file}\n",
    "\n",
    "def update_train_state(args, model, train_state):\n",
    "    \"\"\"Handle the training state updates.\n",
    "\n",
    "    Components:\n",
    "     - Early Stopping: Prevent overfitting.\n",
    "     - Model Checkpoint: Model is saved if the model is better\n",
    "\n",
    "    :param args: main arguments\n",
    "    :param model: model to train\n",
    "    :param train_state: a dictionary representing the training state values\n",
    "    :returns:\n",
    "        a new train_state\n",
    "    \"\"\"\n",
    "\n",
    "    # Save one model at least\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "\n",
    "    # Save model if performance improved\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "\n",
    "        # If loss worsened\n",
    "        if loss_t >= train_state['early_stopping_best_val']:\n",
    "            # Update step\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        # Loss decreased\n",
    "        else:\n",
    "            # Save the best model\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "\n",
    "            # Reset early stopping step\n",
    "            train_state['early_stopping_step'] = 0\n",
    "\n",
    "        # Stop early ?\n",
    "        train_state['stop_early'] = \\\n",
    "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "\n",
    "    return train_state\n",
    "\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100\n",
    "\n",
    "def set_seed_everywhere(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def handle_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)\n",
    "\n",
    "\n",
    "args = Namespace(\n",
    "    # Data and Path information\n",
    "    cbow_csv=\"frankenstein_with_splits.csv\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    model_state_file=\"model.pth\",\n",
    "    save_dir=\"model_storage/ch5/cbow\",\n",
    "    # Model hyper parameters\n",
    "    embedding_size=50,\n",
    "    # Training hyper parameters\n",
    "    seed=1337,\n",
    "    num_epochs=5,\n",
    "    learning_rate=0.0001,\n",
    "    batch_size=32,\n",
    "    early_stopping_criteria=5,\n",
    "    # Runtime options\n",
    "    cuda=True,\n",
    "    catch_keyboard_interrupt=True,\n",
    "    reload_from_files=False,\n",
    "    expand_filepaths_to_save_dir=True\n",
    ")\n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.vectorizer_file = os.path.join(args.save_dir,\n",
    "                                        args.vectorizer_file)\n",
    "\n",
    "    args.model_state_file = os.path.join(args.save_dir,\n",
    "                                         args.model_state_file)\n",
    "\n",
    "    print(\"Expanded filepaths: \")\n",
    "    print(\"\\t{}\".format(args.vectorizer_file))\n",
    "    print(\"\\t{}\".format(args.model_state_file))\n",
    "\n",
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# handle dirs\n",
    "handle_dirs(args.save_dir)\n",
    "\n",
    "if args.reload_from_files:\n",
    "    print(\"Loading dataset and loading vectorizer\")\n",
    "    dataset = CBOWDataset.load_dataset_and_load_vectorizer(args.cbow_csv,\n",
    "                                                           args.vectorizer_file)\n",
    "else:\n",
    "    print(\"Loading dataset and creating vectorizer\")\n",
    "    dataset = CBOWDataset.load_dataset_and_make_vectorizer(args.cbow_csv)\n",
    "    dataset.save_vectorizer(args.vectorizer_file)\n",
    "\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "classifier = CBOWClassifier(vocabulary_size=len(vectorizer.cbow_vocab),\n",
    "                            embedding_size=args.embedding_size)\n",
    "\n",
    "classifier = classifier.to(args.device)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                 mode='min', factor=0.5,\n",
    "                                                 patience=1)\n",
    "train_state = make_train_state(args)\n",
    "\n",
    "epoch_bar = tqdm(desc='training routine',\n",
    "                          total=args.num_epochs,\n",
    "                          position=0)\n",
    "\n",
    "dataset.set_split('train')\n",
    "train_bar = tqdm(desc='split=train',\n",
    "                          total=dataset.get_num_batches(args.batch_size),\n",
    "                          position=1,\n",
    "                          leave=True)\n",
    "dataset.set_split('val')\n",
    "val_bar = tqdm(desc='split=val',\n",
    "                        total=dataset.get_num_batches(args.batch_size),\n",
    "                        position=1,\n",
    "                        leave=True)\n",
    "\n",
    "try:\n",
    "    for epoch_index in range(args.num_epochs):\n",
    "        print(f\"\\nEpoch [{epoch_index + 1}/{args.num_epochs}]\")\n",
    "        print(\"-\" * 50)\n",
    "        train_state['epoch_index'] = epoch_index\n",
    "\n",
    "        # Iterate over training dataset\n",
    "\n",
    "        # setup: batch generator, set loss and acc to 0, set train mode on\n",
    "\n",
    "        dataset.set_split('train')\n",
    "        batch_generator = generate_batches(dataset,\n",
    "                                           batch_size=args.batch_size,\n",
    "                                           device=args.device)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        classifier.train()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "            # the training routine is these 5 steps:\n",
    "\n",
    "            # --------------------------------------\n",
    "            # step 1. zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # step 2. compute the output\n",
    "            y_pred = classifier(x_in=batch_dict['x_data'])\n",
    "\n",
    "            # step 3. compute the loss\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # step 4. use loss to produce gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # step 5. use optimizer to take gradient step\n",
    "            optimizer.step()\n",
    "            # -----------------------------------------\n",
    "            # compute the accuracy\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            if batch_index % 500 == 0:\n",
    "                train_bar.set_postfix(loss=running_loss, acc=running_acc, epoch=epoch_index)\n",
    "            train_bar.update()\n",
    "\n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_acc'].append(running_acc)\n",
    "\n",
    "        # Iterate over val dataset\n",
    "\n",
    "        # setup: batch generator, set loss and acc to 0; set eval mode on\n",
    "        dataset.set_split('val')\n",
    "        batch_generator = generate_batches(dataset,\n",
    "                                           batch_size=args.batch_size,\n",
    "                                           device=args.device)\n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        classifier.eval()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # compute the output\n",
    "            y_pred = classifier(x_in=batch_dict['x_data'])\n",
    "\n",
    "            # step 3. compute the loss\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # compute the accuracy\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            if batch_index % 500 == 0:\n",
    "                val_bar.set_postfix(loss=running_loss, acc=running_acc, epoch=epoch_index)\n",
    "            val_bar.update()\n",
    "\n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)\n",
    "\n",
    "        train_state = update_train_state(args=args, model=classifier,\n",
    "                                         train_state=train_state)\n",
    "\n",
    "        scheduler.step(train_state['val_loss'][-1])\n",
    "\n",
    "        if train_state['stop_early']:\n",
    "            break\n",
    "\n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting loop\")\n",
    "\n",
    "# compute the loss & accuracy on the test set using the best available model\n",
    "\n",
    "classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
    "classifier = classifier.to(args.device)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset,\n",
    "                                   batch_size=args.batch_size,\n",
    "                                   device=args.device)\n",
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "classifier.eval()\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # compute the output\n",
    "    y_pred = classifier(x_in=batch_dict['x_data'])\n",
    "\n",
    "    # compute the loss\n",
    "    loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "    loss_t = loss.item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "    # compute the accuracy\n",
    "    acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc\n",
    "\n",
    "print(\"Test loss: {};\".format(train_state['test_loss']))\n",
    "print(\"Test Accuracy: {}\".format(train_state['test_acc']))\n",
    "\n",
    "\n",
    "def pretty_print(results):\n",
    "    \"\"\"\n",
    "    Pretty print embedding results.\n",
    "    \"\"\"\n",
    "    for item in results:\n",
    "        print(\"...[%.2f] - %s\" % (item[1], item[0]))\n",
    "\n",
    "\n",
    "def get_closest(target_word, word_to_idx, embeddings, n=5):\n",
    "    \"\"\"\n",
    "    Get the n closest\n",
    "    words to your word.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate distances to all other words\n",
    "\n",
    "    word_embedding = embeddings[word_to_idx[target_word.lower()]]\n",
    "    distances = []\n",
    "    for word, index in word_to_idx.items():\n",
    "        if word == \"<MASK>\" or word == target_word:\n",
    "            continue\n",
    "        distances.append((word, torch.dist(word_embedding, embeddings[index])))\n",
    "\n",
    "    results = sorted(distances, key=lambda x: x[1])[1:n + 2]\n",
    "    return results\n",
    "\n",
    "word = 'monster'\n",
    "embeddings = classifier.embedding.weight.data\n",
    "word_to_idx = vectorizer.cbow_vocab._token_to_idx\n",
    "pretty_print(get_closest(word, word_to_idx, embeddings, n=5))\n",
    "\n",
    "target_words = ['frankenstein', 'monster', 'science', 'sickness', 'lonely', 'happy']\n",
    "\n",
    "embeddings = classifier.embedding.weight.data\n",
    "word_to_idx = vectorizer.cbow_vocab._token_to_idx\n",
    "\n",
    "for target_word in target_words:\n",
    "    print(f\"======={target_word}=======\")\n",
    "    if target_word not in word_to_idx:\n",
    "        print(\"Not in vocabulary\")\n",
    "        continue\n",
    "    pretty_print(get_closest(target_word, word_to_idx, embeddings, n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd414e0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
